{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import numpy as np\n",
    "from torch.amp import autocast\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple, Optional, Dict\n",
    "import gzip\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "# from separated_attention import QueryModule, KeyModule, ValueModule, AttentionCombiner\n",
    "# Data handling libraries\n",
    "import torchaudio\n",
    "from einops import rearrange\n",
    "# from torchaudio.transforms import MelSpectrogram\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "# ML libraries\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperTokenizer\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set up environment\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "device = torch.device(device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "torch.set_default_dtype(dtype)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "@dataclass\n",
    "class Dimensions:\n",
    "    vocab: int\n",
    "    text_ctx: int\n",
    "    text_dims: int\n",
    "    text_head: int\n",
    "    text_layerA: int\n",
    "    text_layerB: int\n",
    "    text_act: str\n",
    "    text_debug: int\n",
    "    text_dropout: float\n",
    "    text_checkpoint: bool\n",
    "\n",
    "    mels: int\n",
    "    audio_ctx: int\n",
    "    audio_dims: int\n",
    "    audio_head: int\n",
    "    audio_layerA: int\n",
    "    audio_layerB: int\n",
    "    audio_act: str\n",
    "    audio_debug: int\n",
    "    audio_dropout: float\n",
    "    audio_checkpoint: bool\n",
    "    scale_embedding: float\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight.to(x.dtype),\n",
    "            None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )\n",
    "def _shape(self, tensor: torch.Tensor, ctx: int, batch: int):\n",
    "    return tensor.view(batch, ctx, self.head, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "class ParameterCycler:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "        self.current_idx = 0\n",
    "\n",
    "    def toggle_requires_grad(self):\n",
    "        for i, param in enumerate(self.parameters):\n",
    "            param.requires_grad = i == self.current_idx\n",
    "        self.current_idx = (self.current_idx + 1) % len(self.parameters)\n",
    "\n",
    "def _shape(self, tensor: torch.Tensor, ctx: int, batch: int):\n",
    "    return tensor.view(batch, ctx, self.head, self.head_dim).transpose(1, 2).contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class rotary(nn.Module):\n",
    "    def __init__(self, dims, head, freq=10000, debug=False):\n",
    "        if debug is True:\n",
    "            print(f\"Rotary check: {dims} {head} {freq}\")\n",
    "        super().__init__()\n",
    "        head_dim = dims // head\n",
    "        rot = head_dim // 2\n",
    "        self.dims = dims\n",
    "        self.head = head\n",
    "        self.head_dim = head_dim\n",
    "        self.freq = freq\n",
    "        self.rot = rot\n",
    "        self.dparam = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "        \n",
    "        self.thetas = nn.Parameter(torch.zeros(rot), requires_grad=False)\n",
    "        self.pairs = nn.Parameter(torch.rand(rot, 2) * head_dim, requires_grad=False)\n",
    "        self.tscale = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.rscale = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.matrix = nn.Parameter(torch.eye(head_dim), requires_grad=False)\n",
    "        self.freq_data = 1.0 / (freq ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.invf = nn.Parameter(self.freq_data, requires_grad=False)\n",
    "        \n",
    "        self.cycler = ParameterCycler(parameters=[self.dparam, self.matrix, self.invf, self.thetas, self.pairs, self.tscale, self.rscale])\n",
    "\n",
    "    def vectorize_rotations(self, flat):\n",
    "        self.batch = flat.size(0)\n",
    "        G_matrices = []\n",
    "        for k in range(self.rot):\n",
    "            i, j = self.pairs[k].long()\n",
    "            theta = self.thetas[k] * self.tscale\n",
    "            G = self.rotation_matrix(self.head_dim, i.item(), j.item(), theta)\n",
    "            G_matrices.append(G)\n",
    "        G_combined = torch.eye(self.head_dim, device=flat.device)\n",
    "        for G in G_matrices:\n",
    "            G_combined = G_combined @ G\n",
    "        return flat @ G_combined\n",
    "\n",
    "    def update_freq(self, new_freq):\n",
    "        if new_freq is not None and new_freq != self.freq:\n",
    "            self.freq = new_freq\n",
    "            invf = 1.0 / (self.freq ** (torch.arange(start=0, end=self.hhead_dim, step=2).float() / self.head_dim))\n",
    "            self.invf.data.copy_(invf)\n",
    "            self.update_pairs()\n",
    "\n",
    "    def update_pairs(self):\n",
    "        pairs = []\n",
    "        while len(pairs) < self.rot:\n",
    "            i, j = torch.randint(0, self.h_dim - 1, (2,))\n",
    "            if i != j and (i, j) not in pairs and (j, i) not in pairs:\n",
    "                pairs.append((i, j))\n",
    "        self.pairs.data.copy_(torch.tensor(pairs, dtype=torch.float32))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.matrix)\n",
    "        nn.init.zeros_(self.thetas)\n",
    "\n",
    "    def q_rotation(self, x, theta, u, v):\n",
    "        u = u / torch.linalg.matrix_norm(u, ord=2)\n",
    "        v = v / torch.linalg.matrix_norm(v, ord=2)\n",
    "        cos_ht = torch.cos(theta / 2)\n",
    "        sin_ht = torch.sin(theta / 2)\n",
    "        q = torch.cat([cos_ht.unsqueeze(0), sin_ht * u])\n",
    "        q_conj = torch.cat([cos_ht.unsqueeze(0), -sin_ht * u])  # noqa: F841\n",
    "        x_shape = x.shape\n",
    "        x = x.view(-1, 3)\n",
    "        uv_cross = torch.cross(u.unsqueeze(0), x)\n",
    "        uuv_cross = torch.cross(u.unsqueeze(0), uv_cross)\n",
    "        x_rot = x + 2 * (q[0] * uv_cross + uuv_cross)\n",
    "        x_rot = x_rot.view(*x_shape)\n",
    "        return x_rot\n",
    "\n",
    "    def rotation_matrix(self, dims, i, j, theta):\n",
    "        G = torch.eye(dims, device=theta.device)\n",
    "        c, s = torch.cos(theta), torch.sin(theta)\n",
    "        G[i, i], G[j, j] = c, c\n",
    "        G[i, j], G[j, i] = -s, s\n",
    "        if dims == 3:\n",
    "            u = torch.eye(dims, device=theta.device)[i]\n",
    "            v = torch.eye(dims, device=theta.device)[j]\n",
    "            if theta < 0: \n",
    "                Q = self.q_rotation(torch.eye(dims, device=theta.device), theta=abs(theta), u=u, v=v)\n",
    "            else:\n",
    "                Q = self.q_rotation(torch.eye(dims, device=theta.device), theta=theta, u=u, v=v)\n",
    "            G = (G + Q) / 2\n",
    "        return G\n",
    "\n",
    "    def rotations(self, x): # dparam = nn.Parameter(torch.zeros(1))\n",
    "        direction = torch.sigmoid(self.dparam) * 2 - 1\n",
    "        rotate = int(torch.round(self.rscale * self.rot))\n",
    "        for k in range(rotate):\n",
    "            i, j = self.pairs[k].long()\n",
    "            theta = direction * self.thetas[k] * self.tscale\n",
    "            G = self.rotation_matrix(dims=self.head_dim, i=i.item(), j=j.item(), theta=theta)\n",
    "            x = x @ G\n",
    "        return x\n",
    "\n",
    "    @autocast('cuda', enabled = True)\n",
    "    def forward(self, x):\n",
    "        self.cycler.toggle_requires_grad()\n",
    "        x = x.to(device)\n",
    "        batch, self.ctx, *rest = x.size()\n",
    "\n",
    "        if len(rest) == 1:\n",
    "            self.dims = rest[0]\n",
    "            if self.dims != self.head * self.head_dim:\n",
    "                raise ValueError(\n",
    "                    f\"Needed {self.head * self.head_dim}, but got too many {self.dims}\")\n",
    "        elif len(rest) == 2:\n",
    "            self.head, self.head_dim = rest\n",
    "            if self.head != self.head or self.head_dim != self.head_dim:\n",
    "                raise ValueError(\n",
    "                    f\"This many head {self.head} and head_dims {self.head_dim} we need, got this many head {self.head} and head_dims {self.head_dim} we did.\"\n",
    ")\n",
    "        else:\n",
    "            raise ValueError(f\"Expected the thingy to be 3D or 4D, but got {x.dim()}D\")\n",
    "        x = rearrange(x, 'b s (h d) -> (b s) h d', h=self.head)\n",
    "        x = self.vectorize_rotations(x)\n",
    "        x = x @ self.matrix\n",
    "        x = rearrange(x, '(b s) h d -> b s (h d)', b=batch, h=self.head)\n",
    "        \n",
    "        position = torch.arange(end=self.ctx, device=device, dtype=dtype)\n",
    "        position = rearrange(tensor=position, pattern='s -> s 1')  # [ctx, 1]\n",
    "        div_term = rearrange(tensor=self.invf, pattern='d -> 1 d')  # [1, dim/2]\n",
    "        sinusoid = position * div_term  # [ctx, dim/2]\n",
    "\n",
    "        sin = rearrange(tensor=torch.sin(input=sinusoid), pattern='s d -> 1 s 1 d')  # [1, ctx, 1, dim/2]\n",
    "        cos = rearrange(tensor=torch.cos(input=sinusoid), pattern='s d -> 1 s 1 d')  # [1, ctx, 1, dim/2]\n",
    "        \n",
    "        x = rearrange(tensor=x, pattern='b s (h d) -> b s h d', h=self.head)\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        \n",
    "        x_out = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        x_out = rearrange(x_out, 'b s h d -> b s (h d)')\n",
    "\n",
    "        x_out = x_out * math.sqrt(self.dims)\n",
    "        return x_out\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadA(nn.Module):\n",
    "    use_sdpa = True\n",
    "\n",
    "    def __init__(self, dims: int, head: int, max_dist: int=256):\n",
    "        super().__init__()\n",
    "        self.head = head\n",
    "        self.max_dist = max_dist\n",
    "        self.dims = dims\n",
    "        self.head_dim = dims // head\n",
    "        self.scale = self.head_dim ** -0.25\n",
    "\n",
    "        self.query = Linear(dims, dims)\n",
    "        self.key = Linear(dims, dims, bias=False)\n",
    "        self.value = Linear(dims, dims)\n",
    "        self.out = Linear(dims, dims)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        batch, ctx, _ = x.size()\n",
    "        q = self.query(x)\n",
    "        \n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "    \n",
    "    @autocast('cuda', enabled = True)\n",
    "    def qkv_attention(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        n_batch, ctx, dims = q.shape\n",
    "        scale = (dims // self.head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "    \n",
    "        if  MultiheadA.use_sdpa:\n",
    "            a = scaled_dot_product_attention(\n",
    "                q, k, v, is_causal=mask is not None and ctx > 1\n",
    "            )\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:ctx, :ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "\n",
    "        return out, qk\n",
    "\n",
    "class QueryModule(nn.Module):\n",
    "    \"\"\"Dedicated query projection module that handles only query transformations.\"\"\"\n",
    "    def __init__(self, dims: int, head: int):\n",
    "        super().__init__()\n",
    "        assert dims % head == 0, f\"dims ({dims}) must be divisible by head ({head})\"\n",
    "        self.dims = dims\n",
    "        self.head = head\n",
    "        self.head_dim = dims // head\n",
    "        self.scale = self.head_dim ** -0.25\n",
    "        self.q = Linear(in_features=dims, out_features=dims)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(tensor=self.q.weight, std=0.02)\n",
    "        if self.q.bias is not None:\n",
    "            nn.init.zeros_(tensor=self.q.bias)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch, ctx = x.shape[:2]\n",
    "        q = self.q(x)\n",
    "        \n",
    "        q = q.view(batch, ctx, self.head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        q = q * self.scale\n",
    "        return q\n",
    "\n",
    "class KeyModule(nn.Module):\n",
    "    \"\"\"Dedicated key projection module that handles only key transformations.\"\"\"\n",
    "    \n",
    "    def __init__(self, dims: int, head: int):\n",
    "        \"\"\" Args: dims: Input/output dimension size head: Number of attention head\"\"\"\n",
    "        super().__init__()\n",
    "        assert dims % head == 0, f\"dims ({dims}) must be divisible by head ({head})\"\n",
    "        self.dims = dims\n",
    "        self.head = head\n",
    "        self.head_dim = dims // head\n",
    "        self.scale = self.head_dim ** -0.25\n",
    "        self.key = Linear(in_features=dims, out_features=dims, bias=False)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(tensor=self.key.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch, ctx = x.shape[:2]\n",
    "        k = self.key(x)\n",
    "        k = k.view(batch, ctx, self.head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k * self.scale\n",
    "        return k\n",
    "\n",
    "class ValueModule(nn.Module):\n",
    "    \"\"\"Dedicated value projection module that handles only value transformations.\"\"\"\n",
    "    def __init__(self, dims: int, head: int):\n",
    "    \n",
    "        super().__init__()\n",
    "        assert dims % head == 0, f\"dims ({dims}) must be divisible by head ({head})\"\n",
    "        self.dims = dims\n",
    "        self.head = head\n",
    "        self.head_dim = dims // head\n",
    "        self.value = Linear(in_features=dims, out_features=dims)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(tensor=self.value.weight, std=0.02)\n",
    "        if self.value.bias is not None:\n",
    "            nn.init.zeros_(tensor=self.value.bias)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "      \n",
    "        batch, ctx = x.shape[:2]\n",
    "        v = self.value(x)\n",
    "        v = v.view(batch, ctx, self.head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        return v\n",
    "\n",
    "class KeyValueModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims: int, head: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key_module = KeyModule(dims, head)\n",
    "        self.value_module = ValueModule(dims, head)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        k = self.key_module(x)\n",
    "        v = self.value_module(x)\n",
    "        return k, v\n",
    "\n",
    "\n",
    "class AttentionCombiner(nn.Module):\n",
    "    \"\"\"Combines separate Q and KV representations for attention computation.\"\"\"\n",
    "    use_sdpa = True\n",
    "    def __init__(self, dims: int, head: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dims % head == 0, f\"dims ({dims}) must be divisible by head ({head})\"\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.head = head\n",
    "        self.head_dim = dims // head\n",
    "        \n",
    "        self.out = Linear(in_features=dims, out_features=dims)\n",
    "        nn.init.normal_(tensor=self.out.weight, std=0.02)\n",
    "        nn.init.zeros_(tensor=self.out.bias)\n",
    "\n",
    "    @autocast('cuda', enabled = True)\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q, k, v: Tensors of shape [batch, head, ctx, head_dim] or [batch, ctx, dims]\n",
    "            mask: Optional mask tensor\n",
    "\n",
    "        \"\"\"\n",
    "        if q.dim() == 3:\n",
    "            batch, ctx, _ = q.shape\n",
    "            q = q.view(batch, ctx, self.head, self.head_dim).permute(0, 2, 1, 3)\n",
    "            k = k.view(batch, k.size(1), self.head, self.head_dim).permute(0, 2, 1, 3)\n",
    "            v = v.view(batch, v.size(1), self.head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            batch = q.size(0)\n",
    "            ctx = q.size(2)\n",
    "\n",
    "        if AttentionCombiner.use_sdpa:\n",
    "            try:\n",
    "                attn_output = scaled_dot_product_attention(\n",
    "                    q, k, v, is_causal=mask is not None and ctx > 1\n",
    "                )\n",
    "            except RuntimeError:\n",
    "                print(f\"SDPA failed with shapes: q={q.shape}, k={k.shape}, v={v.shape}\")\n",
    "                attn = torch.matmul(q, k.transpose(-1, -2))\n",
    "                if mask is not None:\n",
    "                    if mask.dim() <= 2:\n",
    "                        mask_to_use = mask[:ctx, :k.size(2)]\n",
    "                        attn = attn + mask_to_use\n",
    "                    else:\n",
    "                        pass\n",
    "                attn = F.softmax(attn, dim=-1)\n",
    "                attn_output = torch.matmul(attn, v)\n",
    "        else:\n",
    "            attn = torch.matmul(q, k.transpose(-1, -2))\n",
    "            \n",
    "            if mask is not None:\n",
    "                if mask.dim() == 2:\n",
    "                    mask_len = min(mask.size(0), ctx)\n",
    "                    mask_to_apply = mask[:mask_len, :mask_len]\n",
    "                    attn[:, :, :mask_len, :mask_len] = attn[:, :, :mask_len, :mask_len] + mask_to_apply\n",
    "                elif mask.dim() == 3:\n",
    "                    mask_len = min(mask.size(1), ctx)\n",
    "                    mask_to_apply = mask[:, :mask_len, :mask_len]\n",
    "                    attn[:, :, :mask_len, :mask_len] = attn[:, :, :mask_len, :mask_len] + mask_to_apply.unsqueeze(1)\n",
    "                elif mask.dim() == 4:\n",
    "                    mask_q_len = min(mask.size(2), ctx)\n",
    "                    mask_k_len = min(mask.size(3), k.size(2))\n",
    "                    attn[:, :, :mask_q_len, :mask_k_len] = attn[:, :, :mask_q_len, :mask_k_len] + mask[:, :, :mask_q_len, :mask_k_len]\n",
    "                \n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn_output = torch.matmul(attn, v)\n",
    "        \n",
    "        output = attn_output.permute(0, 2, 1, 3).reshape(batch, ctx, self.dims)\n",
    "        return self.out(output)\n",
    "\n",
    "\n",
    "class AdaptiveUpdateAttention(nn.Module):\n",
    "    \"\"\"Attention implementation with content-dependent update frequencies.\"\"\"\n",
    "    def __init__(self, dims: int, head: int, max_dist=256):\n",
    "        super().__init__()\n",
    "        self.query_module = QueryModule(dims, head)\n",
    "        self.key_module = KeyModule(dims, head)\n",
    "        self.value_module = ValueModule(dims, head)\n",
    "        self.combiner = AttentionCombiner(dims, head)\n",
    "        self.max_dist = max_dist\n",
    "        self.head = head\n",
    "        self.dims = dims\n",
    "\n",
    "        self.key_update_predictor = nn.Sequential(\n",
    "            Linear(dims, dims // 4), nn.ReLU(), Linear(dims // 4, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.value_update_predictor = nn.Sequential(\n",
    "            Linear(dims, dims // 4), nn.ReLU(), Linear(dims // 4, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.update_threshold = 0.5\n",
    "        self.stored_key_cache = None\n",
    "        self.stored_value_cache = None\n",
    "\n",
    "    def should_update_key(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict whether the key should be updated based on content.\"\"\"\n",
    "        avg_rep = x.mean(dim=1)\n",
    "        return self.key_update_predictor(avg_rep) > self.update_threshold\n",
    "\n",
    "    def should_update_value(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict whether the value should be updated based on content.\"\"\"\n",
    "        avg_rep = x.mean(dim=1)\n",
    "        return self.value_update_predictor(avg_rep) > self.update_threshold\n",
    "\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        \"\"\"Modified forward method to match FocusA's interface\"\"\"\n",
    "        batch, ctx, _ = x.shape\n",
    "        \n",
    "        q = self.query_module(x)\n",
    "        \n",
    "        kv_input = xa if xa is not None else x\n",
    "        device = kv_input.device\n",
    "\n",
    "        if kv_cache is None:\n",
    "            k = self.key_module(kv_input)\n",
    "            v = self.value_module(kv_input)\n",
    "            \n",
    "            self.stored_key_cache = k\n",
    "            self.stored_value_cache = v\n",
    "        else:\n",
    "            update_k = self.should_update_key(kv_input)\n",
    "            update_v = self.should_update_value(kv_input)\n",
    "            \n",
    "            if update_k.any():\n",
    "                new_k = self.key_module(kv_input)\n",
    "                if self.stored_key_cache is not None:\n",
    "                    update_mask = update_k.view(-1, 1, 1, 1).expand_as(self.stored_key_cache)\n",
    "                    k = torch.where(update_mask, new_k, self.stored_key_cache)\n",
    "                else:\n",
    "                    k = new_k\n",
    "            else:\n",
    "                k = self.stored_key_cache if self.stored_key_cache is not None else self.key_module(kv_input)\n",
    "            \n",
    "            if update_v.any():\n",
    "                new_v = self.value_module(kv_input)\n",
    "                if self.stored_value_cache is not None:\n",
    "                    update_mask = update_v.view(-1, 1, 1, 1).expand_as(self.stored_value_cache)\n",
    "                    v = torch.where(update_mask, new_v, self.stored_value_cache)\n",
    "                else:\n",
    "                    v = new_v\n",
    "            else:\n",
    "                v = self.stored_value_cache if self.stored_value_cache is not None else self.value_module(kv_input)\n",
    "            \n",
    "            self.stored_key_cache = k\n",
    "            self.stored_value_cache = v\n",
    "        \n",
    "        output = self.combiner(q, k, v, mask=mask)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Refiner:\n",
    "    def __init__(self, states, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.R = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.default_value = 0.0\n",
    "\n",
    "    def get_value(self, state, action):\n",
    "        return self.R.get((state, action), self.default_value)\n",
    "\n",
    "    def set_value(self, state, action, value):\n",
    "        self.R[(state, action)] = value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.actions)\n",
    "        else:\n",
    "            action_values = [self.get_value(state, a) for a in range(self.actions)]\n",
    "            return np.argmax(action_values)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        next_values = [self.get_value(next_state, a) for a in range(self.actions)]\n",
    "        best_next_value = max(next_values)\n",
    "\n",
    "        old_value = self.get_value(state, action)\n",
    "        td_target = reward + self.gamma * best_next_value\n",
    "        td_error = td_target - old_value\n",
    "        new_value = old_value + self.alpha * td_error\n",
    "        self.set_value(state, action, new_value)\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(in_features=dims, out_features=1)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, global_out):\n",
    "        if global_out.dim() > 2:\n",
    "            global_out = global_out.mean(dim=1)\n",
    "        scale = torch.sigmoid(self.linear(global_out))\n",
    "        \n",
    "        return scale\n",
    "\n",
    "class AdaptiveSpan(nn.Module):\n",
    "    def __init__(self, dims, head, max_dist, sharpen=True, temp_scale=0.01):\n",
    "        super().__init__()\n",
    "        self.head = head\n",
    "        self.max_dist = max_dist\n",
    "        self.dims = dims\n",
    "        self.temp_scale = temp_scale\n",
    "        self.sharpen = sharpen\n",
    "        self.span_scale = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self.head_dim = dims // head\n",
    "        self.register_buffer(\"scale\", torch.tensor(self.head_dim**-0.25))\n",
    "\n",
    "    @autocast('cuda', enabled = True)\n",
    "    def forward(self, query, key, value, max_dist=None, max_span=None, span_scale=None):\n",
    "        if max_dist is None:\n",
    "            max_dist = self.max_dist\n",
    "        if max_span is None:\n",
    "            max_span = query.shape[1]\n",
    "        if span_scale is None:\n",
    "            span_scale = self.span_scale\n",
    "            \n",
    "        span_mean = span_scale.mean().item()\n",
    "        span_len = min(int(max_span * span_mean), query.shape[1], key.shape[1], value.shape[1])\n",
    "        eff_span = min(span_len, max_dist)\n",
    "        \n",
    "        if eff_span == 0:\n",
    "            batch = query.shape[0]\n",
    "            return (torch.zeros(batch, eff_span, self.dims, device=query.device), None)\n",
    "            \n",
    "        q_span = query[:, :eff_span, :]\n",
    "        k_span = key[:, :eff_span, :]\n",
    "        v_span = value[:, :eff_span, :]\n",
    "\n",
    "        batch = q_span.shape[0]\n",
    "\n",
    "        reshape_dims = (batch, -1, self.head, self.head_dim)\n",
    "        q = q_span.view(*reshape_dims).permute(0, 2, 1, 3)\n",
    "        k = k_span.view(*reshape_dims).permute(0, 2, 1, 3)\n",
    "        v = v_span.view(*reshape_dims).permute(0, 2, 1, 3)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
    "            temperature = (\n",
    "                1.0 + self.temp_scale * (1.0 - span_mean)\n",
    "                if self.sharpen\n",
    "                else 0.5 + self.temp_scale * span_mean\n",
    "            )\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "            weights = torch.softmax((scores / temperature) * self.scale, dim=-1)\n",
    "            out = torch.matmul(weights, v)\n",
    "            out = out.permute(0, 2, 1, 3).reshape(batch, eff_span, self.dims)\n",
    "\n",
    "        return out, weights\n",
    "\n",
    "class IntegratedAttention(nn.Module):\n",
    "    def __init__(self, ctx, dims, head, max_dist=512, win_size=256, max_span=384, temp_scale=0.01,):\n",
    "        super().__init__()\n",
    "        self.head = head\n",
    "        self.max_dist = max_dist\n",
    "        self.dims = dims\n",
    "        self.max_span = max_span\n",
    "        self.sliding_window = win_size\n",
    "        self.temp_scale = 0.01\n",
    "        self.sharpen = True\n",
    "        self.head_dim = dims // head\n",
    "        self.batch = None\n",
    "\n",
    "        self.refiner = Refiner(\n",
    "            states=10000, actions=10, alpha=0.1, gamma=0.9, epsilon=0.1\n",
    "        )\n",
    "        self.span_pred = Predictor(dims=dims)\n",
    "        self.attn_local = AdaptiveSpan(\n",
    "            dims=dims, head=head, max_dist=max_dist, sharpen=True, temp_scale=0.01\n",
    "        )\n",
    "\n",
    "        self.attn_global = AdaptiveUpdateAttention(dims=dims, head=head, max_dist=max_dist)\n",
    "        self.projection = Linear(in_features=2 * dims, out_features=dims)\n",
    "\n",
    "        self.ln_a = LayerNorm(normalized_shape=dims)\n",
    "        self.ln_b = LayerNorm(normalized_shape=dims)\n",
    "\n",
    "        mask = torch.empty(max_span, max_span).fill_(float(\"-inf\")).triu_(diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "        self.register_buffer(\"window_mask\", None, persistent=False)\n",
    "        self.register_buffer(\"threshold\", torch.tensor(1e-4), persistent=False)\n",
    "        self.register_buffer(\"s_factor\", torch.tensor(0.1), persistent=False)\n",
    "\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        if mask is None:\n",
    "            mask = self.mask\n",
    "            \n",
    "        local = self.ln_a(x)\n",
    "        globe = self.ln_b(x)\n",
    "\n",
    "        globe_out = self.attn_global(globe, globe, globe)\n",
    "        freq_scale = self.span_pred(globe_out)\n",
    "        state = self.extract(local)\n",
    "\n",
    "        action = self.refiner.choose_action(state=state)\n",
    "        refine = self.action_scale(action=action)\n",
    "\n",
    "        span_scale = torch.clamp(freq_scale * refine, min=0.0, max=1.0)\n",
    "        span_mean = span_scale.mean().item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            current_win_size = max(1, int(self.sliding_window * span_mean))\n",
    "            current_span_len = max(1, int(self.max_span * span_mean))\n",
    "\n",
    "            effective_max = min(self.max_dist, local.size(1))\n",
    "            local_max = min(self.max_dist, current_span_len, current_win_size)\n",
    "            globe_max = effective_max\n",
    "\n",
    "        self.attn_local.max_dist = local_max\n",
    "        self.attn_global.max_dist = globe_max\n",
    "\n",
    "        local_out = self.slide_win(\n",
    "            x=local,\n",
    "            win_size=current_win_size,\n",
    "            span_len=current_span_len,\n",
    "            span_scale=span_scale,\n",
    "            mask=mask,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            quality = self.quality(output=local_out)\n",
    "            next_state = self.extract(local_out)\n",
    "            self.refiner.update(\n",
    "                state=state, action=action, reward=quality, next_state=next_state)\n",
    "        combined = torch.cat([local_out, globe_out], dim=-1)\n",
    "        x = self.projection(combined)\n",
    "        return x\n",
    "\n",
    "    def quality(self, output):\n",
    "        with torch.no_grad():\n",
    "            safe_output = output.clamp(min=1e-10)\n",
    "            entropy = -(safe_output * torch.log(safe_output)).sum(-1).mean()\n",
    "            coverage = (output > 0.01).float().mean()\n",
    "            return float(coverage - 0.1 * entropy)\n",
    "\n",
    "    def extract(self, x):\n",
    "        with torch.no_grad():\n",
    "            mean_state = x.mean(dim=(0, 1))\n",
    "            var_state = x.var(dim=(0, 1), unbiased=False)\n",
    "            state = torch.cat([mean_state, var_state])\n",
    "            state_id = self.discretize(state.cpu().numpy())\n",
    "        return state_id\n",
    "\n",
    "    def discretize(self, state):\n",
    "        bins = np.linspace(-1, 1, num=10)\n",
    "        state_discrete = np.digitize(state, bins)\n",
    "        state_hash = hash(tuple(state_discrete))\n",
    "        state_id = state_hash % (self.refiner.states - 1)\n",
    "        return state_id\n",
    "\n",
    "    def action_scale(self, action):\n",
    "        span_value = action / (self.refiner.actions - 1)\n",
    "        device = next(self.parameters()).device\n",
    "        dtype = next(self.parameters()).dtype\n",
    "        span_scale = torch.tensor([span_value], device=device, dtype=dtype)\n",
    "        return span_scale\n",
    "    \n",
    "    @autocast('cuda', enabled = True)\n",
    "    def _focus(self, query, key, value, span_scale, mask):\n",
    "        max_iterations = 10\n",
    "        iteration = 0\n",
    "        prev_attn = torch.zeros_like(input=query)\n",
    "        attn_out = torch.zeros_like(input=query)\n",
    "        attn_weights = None\n",
    "\n",
    "        threshold = self.threshold.item()\n",
    "        s_factor = self.s_factor.item()\n",
    "\n",
    "        while iteration < max_iterations:\n",
    "            span_len = int(self.max_span * span_scale.mean().item())\n",
    "            span_len = min(span_len, query.size(1), key.size(1), value.size(1))\n",
    "            eff_span = min(span_len, self.max_dist)\n",
    "\n",
    "            if eff_span == 0:\n",
    "                break\n",
    "\n",
    "            q_span = query[:, :eff_span, :]\n",
    "            k_span = key[:, :eff_span, :]\n",
    "            v_span = value[:, :eff_span, :]\n",
    "\n",
    "            batch, ctx, dims = q_span.size()\n",
    "            d_k = dims // self.head\n",
    "            scale_factor = 1 / math.sqrt(d_k)\n",
    "\n",
    "            q = q_span.view(batch, ctx, self.head, -1).transpose(1, 2)\n",
    "            k = k_span.view(batch, ctx, self.head, -1).transpose(1, 2)\n",
    "            v = v_span.view(batch, ctx, self.head, -1).transpose(1, 2)\n",
    "\n",
    "            if self.sharpen:\n",
    "                temperature = 1.0 + self.temp_scale * (1.0 - span_scale.mean().item())\n",
    "            else:\n",
    "                temperature = 0.5 + self.temp_scale * span_scale.mean().item()\n",
    "            attn_scores = (\n",
    "                torch.matmul(q, k.transpose(-2, -1)) * scale_factor / temperature\n",
    "            )\n",
    "            if mask.size(-2) != attn_scores.size(-2) or mask.size(\n",
    "                -1\n",
    "            ) != attn_scores.size(-1):\n",
    "\n",
    "                mask_q_len = min(mask.size(-2), attn_scores.size(-2))\n",
    "                mask_k_len = min(mask.size(-1), attn_scores.size(-1))\n",
    "                resized_mask = torch.ones(\n",
    "                    (\n",
    "                        batch,\n",
    "                        self.head,\n",
    "                        attn_scores.size(-2),\n",
    "                        attn_scores.size(-1),\n",
    "                    ),\n",
    "                    device=mask.device,\n",
    "                    dtype=mask.dtype,\n",
    "                )\n",
    "                resized_mask[:, :, :mask_q_len, :mask_k_len] = mask[\n",
    "                    :, :, :mask_q_len, :mask_k_len\n",
    "                ]\n",
    "                mask = resized_mask\n",
    "\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "            attn_out = torch.matmul(attn_weights, v)\n",
    "            attn_out = (\n",
    "                attn_out.transpose(1, 2).contiguous().view(batch, ctx, -1)\n",
    "            )\n",
    "\n",
    "            diff = torch.abs(attn_out - prev_attn).mean()\n",
    "            dynamic_threshold = threshold + s_factor * diff\n",
    "\n",
    "            if diff < dynamic_threshold:\n",
    "                break\n",
    "\n",
    "            prev_attn = attn_out\n",
    "            query = query + attn_out\n",
    "            iteration += 1\n",
    "        return attn_out, attn_weights\n",
    "    \n",
    "    @autocast('cuda', enabled = True)\n",
    "    def slide_win(self, x, win_size, span_len, span_scale, mask):\n",
    "        batch, ctx, dims = x.size()\n",
    "        self.batch = batch\n",
    "        num_windows = (ctx + win_size - 1) // win_size\n",
    "        output = torch.zeros_like(x)\n",
    "        device = x.device\n",
    "        default_mask = None\n",
    "\n",
    "        for i in range(num_windows):\n",
    "            start_idx = i * win_size\n",
    "            end_idx = min((i + 1) * win_size, ctx)\n",
    "            window_size = end_idx - start_idx\n",
    "\n",
    "            key_start = max(0, start_idx - span_len + win_size)\n",
    "            key_end = min(start_idx + span_len, ctx)\n",
    "            span_size = key_end - key_start\n",
    "\n",
    "            query = x[:, start_idx:end_idx, :]\n",
    "            key = x[:, key_start:key_end, :]\n",
    "            value = key\n",
    "\n",
    "            if mask is not None:\n",
    "                if mask.dim() == 4:\n",
    "                    window_mask = mask[:, :, start_idx:end_idx, key_start:key_end]\n",
    "                    if window_mask.size(1) == 1:\n",
    "                        window_mask = window_mask.expand(-1, self.head, -1, -1)\n",
    "                else:\n",
    "                    if (\n",
    "                        default_mask is None\n",
    "                        or default_mask.size(-2) != window_size\n",
    "                        or default_mask.size(-1) != span_size\n",
    "                    ):\n",
    "                        default_mask = torch.ones(\n",
    "                            (batch, self.head, window_size, span_size),\n",
    "                            device=device,\n",
    "                            dtype=torch.bool,\n",
    "                        )\n",
    "                    window_mask = default_mask\n",
    "            else:\n",
    "                if (\n",
    "                    default_mask is None\n",
    "                    or default_mask.size(-2) != window_size\n",
    "                    or default_mask.size(-1) != span_size\n",
    "                ):\n",
    "                    default_mask = torch.ones(\n",
    "                        (batch, self.head, window_size, span_size),\n",
    "                        device=device,\n",
    "                        dtype=torch.bool,\n",
    "                    )\n",
    "                window_mask = default_mask\n",
    "\n",
    "            attn_out, _ = self._focus(\n",
    "                query=query,\n",
    "                key=key,\n",
    "                value=value,\n",
    "                span_scale=span_scale,\n",
    "                mask=window_mask,\n",
    "            )\n",
    "\n",
    "            output[:, start_idx:end_idx, :] = attn_out\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, dims: int, head: int, dropout: float, act: str, debug=False, cross_attention=False):\n",
    "        if debug is True:\n",
    "            print(f\"Residual check:{dims} {head} {dropout} {act}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.head = head\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.cross_attention = cross_attention\n",
    "\n",
    "        act_map = {\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(),\n",
    "            \"elu\": nn.ELU(),\n",
    "        }\n",
    "        act = act_map.get(act, nn.GELU())\n",
    "        \n",
    "        self.attna = MultiheadA(dims=dims, head=head)\n",
    "        #self.attnb = IntegratedAttention(dims=dims, head=head) if IntegratedAttention else None\n",
    "        self.attnc = MultiheadA(dims=dims, head=head) if cross_attention else None\n",
    "    \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            Linear(in_features=dims, out_features=dims * 4, bias=True),\n",
    "            act,\n",
    "            nn.Dropout(p=dropout),\n",
    "            Linear(in_features=dims * 4, out_features=dims, bias=True))\n",
    "\n",
    "        self.lna = LayerNorm(normalized_shape=dims)\n",
    "        self.lnb = LayerNorm(normalized_shape=dims) \n",
    "        self.lnc = LayerNorm(normalized_shape=dims) if cross_attention else None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        x = x + self.attna(self.lna(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.attnc is not None:\n",
    "            x = x + self.attnc(self.lnc(x), xa, kv_cache=kv_cache)[0] # type: ignore # head mask revisit\n",
    "        x = x + self.mlp(self.lnb(x))\n",
    "        return x\n",
    "    \n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__( self, mels: int, ctx: int, dims: int, head: int, layerA: int, layerB: int, checkpoint: bool, dropout: float, act: str,  \n",
    "                 scale_embedding, debug=None):\n",
    "        if debug == 1:\n",
    "            print(\n",
    "                f\"AudioEncoder check: {mels} {ctx} {dims} {head} {checkpoint} {dropout} {act} {layerA} {layerB}\"\n",
    "            )\n",
    "        super().__init__()\n",
    "        self.ctx = ctx\n",
    "        self.dims = dims\n",
    "        self.head = head\n",
    "        self.layerA = layerA\n",
    "        self.layerB = layerB\n",
    "        self.checkpoint = checkpoint\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.scale_embedding = scale_embedding\n",
    "        self.rotary = rotary(dims=dims, head=head)\n",
    "\n",
    "        act_map = {\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"leaky_relu\": nn.LeakyReLU(),\n",
    "            \"elu\": nn.ELU(),\n",
    "        }\n",
    "        act = act_map.get(act, nn.GELU())\n",
    "\n",
    "        self.conv1 = Conv1d(mels, dims, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(dims, dims, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(ctx, dims))\n",
    "\n",
    "        self.blockA = ( nn.ModuleList( modules=[ Residual(dims=dims, head=head, dropout=dropout, act=act, debug=debug) \n",
    "                                                for _ in range(layerA) ] ) if layerA > 0 else None )\n",
    "\n",
    "        self.blockB = ( nn.ModuleList( modules=[ IntegratedAttention(ctx=ctx, dims=dims, head=head)\n",
    "                                                for _ in range(layerB) ] ) if layerB > 0 else None )\n",
    "        self.ln_enc = LayerNorm(dims)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x + self.rotary(x)\n",
    "        x = x * self.scale_embedding\n",
    "\n",
    "        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        x = ( checkpoint(self._forward, x, use_reentrant=True) if self.checkpoint else x ) \n",
    "\n",
    "        for block in chain(self.blockA or [], self.blockB or []):\n",
    "            if self.checkpoint:\n",
    "                x = checkpoint(block, x, use_reentrant=True)\n",
    "            else:\n",
    "                result = block(x)\n",
    "                if isinstance(result, tuple):\n",
    "                    x = result[0]\n",
    "                else:\n",
    "                    x = result\n",
    "        x = self.ln_enc(x)\n",
    "        return x\n",
    "    \n",
    "    def _forward(self, x: Tensor):\n",
    "        return x\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__( self, vocab: int, ctx: int, dims: int, head: int, layerA: int, layerB: int, checkpoint: bool, dropout: float, act: str,  debug=None):\n",
    "        if debug == 2: print( f\"TextDecoder check: {vocab} {ctx} {dims} {head} {checkpoint} {dropout} {act} {layerA} {layerB}\" )  # noqa: E701\n",
    "        super().__init__() \n",
    "        self.checkpoint = checkpoint \n",
    "        self.dims = dims \n",
    "        self.head = head \n",
    "        self.layerA = layerA \n",
    "        self.layerB = layerB \n",
    "        self.dropout = dropout \n",
    "        self.act = act \n",
    "        \n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab, embedding_dim=dims) \n",
    "        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02) \n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(data=torch.empty(ctx, dims)) \n",
    "        nn.init.normal_(tensor=self.positional_embedding, mean=0.0, std=0.02) \n",
    "\n",
    "        self.ln_dec = LayerNorm(normalized_shape=dims) \n",
    "        \n",
    "        self.blockA = ( nn.ModuleList( modules=[ Residual(dims=dims, head=head, dropout=dropout, act=act ) \n",
    "                                                for _ in range(layerA) ] ) if layerA > 0 else None ) \n",
    "        \n",
    "        self.blockB = ( nn.ModuleList( modules=[ IntegratedAttention(ctx=ctx, dims=dims, head=head)\n",
    "                                                for _ in range(layerB) ] ) if layerB > 0 else None )\n",
    "        \n",
    "        mask = torch.empty(ctx, ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        x = (\n",
    "            self.token_embedding(x)\n",
    "            + self.positional_embedding[offset : offset + x.shape[-1]]\n",
    "        )\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in chain(self.blockA or [], self.blockB or []):\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "\n",
    "        x = self.ln_dec(x)\n",
    "        logits = (\n",
    "            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)\n",
    "        ).float()\n",
    "        return logits\n",
    "\n",
    "class Echo(nn.Module):\n",
    "    def __init__(self, param: Dimensions, debug=None):\n",
    "        super().__init__()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.debug = debug\n",
    "        self.param = param\n",
    "        self.to(device)\n",
    "\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.param.mels,\n",
    "            self.param.audio_ctx,\n",
    "            self.param.audio_dims,\n",
    "            self.param.audio_head,\n",
    "            self.param.audio_layerA,\n",
    "            self.param.audio_layerB,\n",
    "            self.param.audio_checkpoint,\n",
    "            self.param.audio_dropout,\n",
    "            self.param.audio_act,\n",
    "            self.param.scale_embedding,\n",
    "\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.param.vocab,\n",
    "            self.param.text_ctx,\n",
    "            self.param.text_dims,\n",
    "            self.param.text_head,\n",
    "            self.param.text_layerA,\n",
    "            self.param.text_layerB,\n",
    "            self.param.text_checkpoint,\n",
    "            self.param.text_dropout,\n",
    "            self.param.text_act,\n",
    "        )\n",
    "\n",
    "        all_head = torch.zeros(\n",
    "            self.param.text_layerA, self.param.text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_head[self.param.text_layerA // 2 :] = True\n",
    "        self.register_buffer(\"alignment_head\", all_head.to_sparse(), persistent=False)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        self.init_counts = {\"Linear\": 0, \"Conv1d\": 0, \"LayerNorm\": 0, \"Embedding\": 0}\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "                self.init_counts[\"Linear\"] += 1\n",
    "            if isinstance(module, Conv1d):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "                self.init_counts[\"Conv1d\"] += 1\n",
    "            if isinstance(module, LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "                self.init_counts[\"LayerNorm\"] += 1\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "                self.init_counts[\"Embedding\"] += 1\n",
    "                \n",
    "    def init_weights(self):  # noqa: F811\n",
    "        print(\"Initializing all weights\")\n",
    "        self.apply(self._init_weights)\n",
    "        print(\"Initialization summary:\")\n",
    "        for module_type, count in self.init_counts.items():\n",
    "            print(f\"{module_type}: {count}\")\n",
    "\n",
    "    def set_alignment_head(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.param.text_layerA, self.param.text_head\n",
    "        )\n",
    "        self.register_buffer(\"alignment_head\", mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, input_features: torch.Tensor):\n",
    "        return self.encoder(input_features)\n",
    "\n",
    "    def logits(self,input_ids: torch.Tensor, audio_features: torch.Tensor):\n",
    "        return self.decoder(input_ids, audio_features)\n",
    "\n",
    "    def forward(self, mel: torch.Tensor, tokens: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        loss = None\n",
    "        if self.training:\n",
    "            audio_features = self.encoder(mel)\n",
    "            logits = self.decoder(tokens, audio_features)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), tokens.view(-1))\n",
    "            self.update_freq(loss=loss.item())\n",
    "        else:\n",
    "            audio_features = self.encoder(mel)\n",
    "            logits = self.decoder(tokens, audio_features)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.param.vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.param.vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.param.text_ctx:\n",
    "    \n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def save_adaptive_output(module, _, output):\n",
    "            if isinstance(output, tuple) and len(output) == 2:\n",
    "                tensor_output, cache_updates = output\n",
    "                \n",
    "                module_key = f\"{module}_key\"\n",
    "                module_value = f\"{module}_value\"\n",
    "                \n",
    "                if module_key not in cache or tensor_output.shape[1] > self.param.text_ctx:\n",
    "                    cache[module_key] = cache_updates[\"key_cache\"]\n",
    "                    cache[module_value] = cache_updates[\"value_cache\"]\n",
    "                else:\n",
    "                    cache[module_key] = torch.cat([cache[module_key], cache_updates[\"key_cache\"]], dim=1).detach()\n",
    "                    cache[module_value] = torch.cat([cache[module_value], cache_updates[\"value_cache\"]], dim=1).detach()\n",
    "\n",
    "                return tensor_output\n",
    "            return output\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiheadA):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "            elif isinstance(layer, AdaptiveUpdateAttention):\n",
    "                hooks.append(layer.register_forward_hook(save_adaptive_output))\n",
    "\n",
    "        # Apply hooks to both encoder and decoder if needed\n",
    "        if self.blockB and any(isinstance(m, AdaptiveUpdateAttention) for m in self.blockB):\n",
    "            self.encoder.apply(install_hooks)\n",
    "        self.decoder.apply(install_hooks)\n",
    "        \n",
    "        return cache, hooks\n",
    "    \n",
    "    def adjust_freq(self, loss, factor=1.0025) -> float | int:\n",
    "            if self.adjust_counter % 25 == 0:\n",
    "                if loss < self.best_loss:\n",
    "                    new_freq=self.freq*factor\n",
    "                else:\n",
    "                    new_freq=self.freq/factor\n",
    "                self.update_freq(new_freq=new_freq)\n",
    "                self.freq=new_freq\n",
    "                self.best_loss=loss\n",
    "            self.adjust_counter += 1\n",
    "            return self.freq\n",
    "            \n",
    "    def update_freq(self, new_freq):\n",
    "        self.new_freq=new_freq\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            if isinstance(module, (rotary)):\n",
    "                module.update_freq(new_freq=self.new_freq)\n",
    "\n",
    "    def generate(self, mel: torch.Tensor, max_length: int = 512) -> torch.Tensor:\n",
    "        audio_features = self.encoder(mel)\n",
    "        return self.decoder.generate(audio_features, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ctx_to_samples(audio_ctx, hop_length):\n",
    "    samples_token = hop_length * 2\n",
    "    n_samples = audio_ctx * samples_token\n",
    "    return n_samples\n",
    "\n",
    "def load_wave(wave_data, sample_rate):\n",
    "    if isinstance(wave_data, str):\n",
    "        waveform, sr = torchaudio.load(uri=wave_data, normalize=False)\n",
    "    elif isinstance(wave_data, dict):\n",
    "        waveform = torch.tensor(data=wave_data[\"array\"]).float()\n",
    "        sr = wave_data[\"sampling_rate\"]\n",
    "    else:\n",
    "        raise TypeError(\"Invalid wave_data format.\")\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)(\n",
    "            waveform\n",
    "        )\n",
    "    return waveform.flatten()\n",
    "\n",
    "def pad(array, target_length, axis=-1, dtype: torch.dtype = torch.float32):\n",
    "    if isinstance(array, np.ndarray):\n",
    "        array = torch.from_numpy(ndarray=array).to(dtype=dtype)\n",
    "    if torch.is_tensor(array):\n",
    "        if array.shape[axis] > target_length:\n",
    "            array = array.index_select(\n",
    "                dim=axis,\n",
    "                index=torch.arange(\n",
    "                    end=target_length, device=array.device, dtype=torch.long\n",
    "                ),\n",
    "            )\n",
    "        if array.shape[axis] < target_length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, target_length - array.shape[axis])\n",
    "            array = F.pad(\n",
    "                input=array, pad=[pad for sizes in pad_widths[::-1] for pad in sizes]\n",
    "            )\n",
    "        array = array.to(dtype=dtype)\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f\"Unsupported input type: {type(array)}. Expected torch.Tensor or np.ndarray.\"\n",
    "        )\n",
    "    return array\n",
    "\n",
    "def exact_div(x, y):\n",
    "    assert x % y == 0\n",
    "    return x // y\n",
    "\n",
    "def process_audio(audio, audio_ctx, mels, hop_length, n_fft, sr):\n",
    "\n",
    "    audio = load_wave(wave_data=audio, sample_rate=sr)\n",
    "    n_samples = ctx_to_samples(audio_ctx=audio_ctx, hop_length=hop_length)\n",
    "    audio = pad(array=audio, target_length=n_samples)\n",
    "\n",
    "    transform = T.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=mels,\n",
    "        norm='slaney',\n",
    "        normalized=True,\n",
    "        power=2.0,\n",
    "        center=True, \n",
    "        window_fn=torch.hann_window,\n",
    "    )\n",
    "    \n",
    "    mel_spectrogram = transform(audio)\n",
    "\n",
    "    target_frames = exact_div(n_samples, hop_length) \n",
    "    mel_spectrogram = pad(array=mel_spectrogram, target_length=target_frames, axis=-1)\n",
    "\n",
    "    log_mel = torch.clamp(mel_spectrogram, min=1e-10).log10()\n",
    "    log_mel = torch.maximum(log_mel, log_mel.max() - 8.0)\n",
    "    log_mel = (log_mel + 4.0) / 4.0\n",
    "    \n",
    "    return log_mel\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"openai/whisper-small\"\n",
    ")\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, tokenizer, audio_ctx, text_ctx, mels, n_fft=1024, hop_length=160, sample_rate=16000, device=\"cpu\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_ctx = text_ctx\n",
    "        self.audio_ctx = audio_ctx\n",
    "        self.sample_rate = sample_rate\n",
    "        self.mels = mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.device = device\n",
    "        self.decoder_start_token_id = tokenizer.bos_token_id \n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = len(features)\n",
    "\n",
    "        max_time_frames = (\n",
    "            ctx_to_samples(audio_ctx=self.audio_ctx, hop_length=self.hop_length)\n",
    "            // self.hop_length\n",
    "        )\n",
    "        batch_audio = torch.zeros(\n",
    "            size=(batch, self.mels, max_time_frames),\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "        batch_input_ids = torch.full(\n",
    "            size=(batch, self.text_ctx),\n",
    "            fill_value=self.pad_token_id,\n",
    "            dtype=torch.long,\n",
    "            device=self.device,\n",
    "        )\n",
    "        batch_labels = torch.full(\n",
    "            size=(batch, self.text_ctx),\n",
    "            fill_value=-100,\n",
    "            dtype=torch.long,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        for i, feature in enumerate(iterable=features):\n",
    "\n",
    "            audio = process_audio(\n",
    "                audio=feature[\"audio\"],\n",
    "                audio_ctx=self.audio_ctx,\n",
    "                mels=self.mels,\n",
    "                hop_length=self.hop_length,\n",
    "                n_fft=self.n_fft,\n",
    "                sr=self.sample_rate,\n",
    "            )\n",
    "            time_frames = audio.shape[-1]\n",
    "            transcript = feature[\"transcription\"]\n",
    "            encoded_input = self.tokenizer.encode(transcript)\n",
    "            encoded_label = self.tokenizer.encode(transcript)\n",
    "            decoder_input = [self.decoder_start_token_id] + encoded_input\n",
    "            labels = encoded_label + [self.tokenizer.eos_token_id]\n",
    "            decoder_input = decoder_input[: self.text_ctx] + [self.pad_token_id] * (\n",
    "                self.text_ctx - len(decoder_input)\n",
    "            )\n",
    "            labels = labels[: self.text_ctx] + [-100] * (self.text_ctx - len(labels))\n",
    "            batch_input_ids[i, : len(decoder_input)] = torch.tensor(data=decoder_input, dtype=torch.long)\n",
    "            batch_labels[i, : len(labels)] = torch.tensor(data=labels, dtype=torch.long)\n",
    "            batch_audio[i, :, :time_frames] = torch.tensor(data=audio, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"input_features\": batch_audio,\n",
    "            \"input_ids\": batch_input_ids,\n",
    "            \"labels\": batch_labels,\n",
    "        }\n",
    "\n",
    "metric = evaluate.load(path=\"wer\")\n",
    "\n",
    "def compute_metrics(pred, tokenizer):\n",
    "    pred_ids = pred[\"predictions\"]\n",
    "    label_ids = pred[\"label_ids\"]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    else:\n",
    "        pred_ids = pred_ids\n",
    "    if pred_ids.ndim == 3:\n",
    "        pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str) # type: ignore\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_evaluate(model, tokenizer, train_loader, eval_loader, optimizer, scheduler, loss_fn,\n",
    "                        max_steps=10000, device='cuda', accumulation_steps=1, clear_cache=True,\n",
    "                        log_interval=10, eval_interval=100, save_interval=1000,\n",
    "                        checkpoint_dir=\"checkpoint_dir\", log_dir=\"log_dir\"):\n",
    "    model.to(device)\n",
    "    global_step = 0\n",
    "    scaler = torch.GradScaler()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    train_iterator = iter(train_loader)\n",
    "    total_loss = 0\n",
    "    step_in_report = 0\n",
    "    dataset_epochs = 0\n",
    "\n",
    "    progress_bar = tqdm(total=max_steps, desc=\"Training Progress\", leave=True, colour='green')\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    while global_step < max_steps:\n",
    "        try:\n",
    "            batch = next(train_iterator)\n",
    "        except StopIteration:\n",
    "            train_iterator = iter(train_loader)\n",
    "            batch = next(train_iterator)\n",
    "            dataset_epochs += 1\n",
    "            print(f\"Starting dataset epoch {dataset_epochs}\")\n",
    "\n",
    "            if step_in_report > 0:\n",
    "                avg_loss = total_loss / step_in_report\n",
    "                logging.info(f\"Dataset iteration complete - Steps: {global_step}, Avg Loss: {avg_loss:.4f}\")\n",
    "                total_loss = 0\n",
    "                step_in_report = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        input_features = batch['input_features'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].long().to(device)\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            input_features_encoded = model.encoder(input_features)\n",
    "            decoder_output = model.decoder(input_ids, input_features_encoded)\n",
    "            logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "            active_logits = logits.view(-1, decoder_output.size(-1))\n",
    "            active_labels = labels.view(-1)\n",
    "            active_mask = active_labels != -100\n",
    "            active_logits = active_logits[active_mask]\n",
    "            active_labels = active_labels[active_mask]\n",
    "            loss = loss_fn(active_logits, active_labels)\n",
    "            # model.adjust_freq(loss=loss.item())\n",
    "        total_loss += loss.item()\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (global_step + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if clear_cache:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        end_time = time.time()\n",
    "        samples_per_sec = len(batch['input_features']) / (end_time - start_time)\n",
    "\n",
    "        if global_step % log_interval == 0:\n",
    "            writer.add_scalar(tag='Loss/train', scalar_value=total_loss / (global_step + 1), global_step=global_step)\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            writer.add_scalar(tag='LearningRate', scalar_value=lr, global_step=global_step)\n",
    "            writer.add_scalar(tag='SamplesPerSec', scalar_value=samples_per_sec, global_step=global_step)\n",
    "\n",
    "        if global_step % eval_interval == 0:\n",
    "            model.eval()\n",
    "            eval_start_time = time.time()\n",
    "            eval_loss = 0\n",
    "            all_predictions = []\n",
    "            all_labels = []\n",
    "            batch_count = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for eval_batch in eval_loader:\n",
    "                # for eval_batch in tqdm(eval_loader, desc=f\"Evaluating (Step {global_step})\", leave=True, colour='green'):\n",
    "                    input_features = eval_batch['input_features'].to(device)\n",
    "                    input_ids = eval_batch['input_ids'].to(device)\n",
    "                    labels = eval_batch['labels'].long().to(device)\n",
    "\n",
    "                    batch = input_features.size(0)\n",
    "                    total_samples += batch\n",
    "\n",
    "                    input_features_encoded = model.encoder(input_features)\n",
    "                    decoder_output = model.decoder(input_ids, input_features_encoded)\n",
    "                    logits = decoder_output.view(-1, decoder_output.size(-1))\n",
    "                    loss = loss_fn(logits, labels.view(-1))\n",
    "                    eval_loss += loss.item()\n",
    "                    all_predictions.extend(torch.argmax(decoder_output, dim=-1).cpu().numpy().tolist())\n",
    "                    all_labels.extend(labels.cpu().numpy().tolist())\n",
    "                    batch_count += 1\n",
    "\n",
    "            eval_time = time.time() - eval_start_time\n",
    "            loss_avg = eval_loss / batch_count if batch_count > 0 else 0\n",
    "            predictions = {\"predictions\": np.array(all_predictions, dtype=object), \"label_ids\": np.array(all_labels, dtype=object)}\n",
    "            metrics = compute_metrics(pred=predictions, tokenizer=tokenizer)\n",
    "\n",
    "            writer.add_scalar('Loss/eval', loss_avg, global_step)\n",
    "            writer.add_scalar('WER', metrics['wer'], global_step)\n",
    "            writer.add_scalar('EvalSamples', total_samples, global_step)\n",
    "            writer.add_scalar('EvalTimeSeconds', eval_time, global_step)\n",
    "\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(f\" • WER:{metrics['wer']:.2f}% • Loss:{loss_avg:.4f} • LR:{lr:.8f}\")\n",
    "\n",
    "            logging.info(f\"EVALUATION STEP {global_step} - WER: {metrics['wer']:.2f}%, Loss: {loss_avg:.4f}, LR: {lr:.8f}\")\n",
    "            #scheduler.step()\n",
    "            model.train()\n",
    "\n",
    "        if global_step % save_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{global_step}.pt')\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            #print(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "            logging.info(f\"Model saved at step {global_step} to {checkpoint_path}\")\n",
    "            \n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "        step_in_report += 1\n",
    "        \n",
    "        avg_loss = total_loss / (global_step + 1)\n",
    "        postfix_dict = {\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'lr': f'{lr:.6f}',\n",
    "            'WER': f'{metrics[\"wer\"]:.4f}' if 'wer' in metrics else 'N/A',\n",
    "            'samp/sec': f'{samples_per_sec:.1f}'\n",
    "        }\n",
    "        progress_bar.set_postfix(postfix_dict, refresh=True)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    final_model_path = os.path.join(checkpoint_dir, 'final_model.pt')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Training completed after {global_step} steps. Final model saved to {final_model_path}\")\n",
    "    writer.close()\n",
    "    progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    checkpoint_dir = './output/checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    log_dir = os.path.join(\"./output/logs\", datetime.now().strftime(format=\"%m-%d_%H\"))\n",
    "    os.makedirs(name=log_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(log_dir, 'training.log'), filemode='w', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "    token = \"\"\n",
    "    dataset = load_dataset(\n",
    "        path=\"google/fleurs\",\n",
    "        name=\"en_us\",\n",
    "        streaming=False,\n",
    "        token=token,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=\"E:/cache\",\n",
    "    ).select_columns(column_names=[\"audio\", \"transcription\"])\n",
    "   \n",
    "    debug = None\n",
    "    param = Dimensions(\n",
    "        mels=128,\n",
    "        audio_ctx=1500,\n",
    "        audio_head=4,\n",
    "        audio_layerA=4,\n",
    "        audio_layerB=2,\n",
    "        audio_dims=512,\n",
    "        audio_act=\"gelu\",\n",
    "        audio_checkpoint=False,\n",
    "        audio_dropout=0.001,\n",
    "        scale_embedding=1.0,\n",
    "        audio_debug = debug,\n",
    "        vocab=51865,\n",
    "        text_ctx=448,\n",
    "        text_head=4,\n",
    "        text_layerA=4,\n",
    "        text_layerB=0,\n",
    "        text_dims=512,\n",
    "        text_act=\"gelu\",\n",
    "        text_checkpoint=False,\n",
    "        text_dropout=0.001,\n",
    "        text_debug = debug,\n",
    "         )\n",
    "    \n",
    "    model = Echo(param=param).to('cuda')\n",
    "    model.init_weights()\n",
    "\n",
    "    DataCollator = DataCollator(tokenizer=tokenizer, \n",
    "                                audio_ctx=param.audio_ctx, \n",
    "                                text_ctx=param.text_ctx, \n",
    "                                mels=param.mels, \n",
    "                                device='cuda')\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=dataset[\"train\"], \n",
    "        batch_size=1, \n",
    "        collate_fn=DataCollator,\n",
    "        num_workers=0)\n",
    "\n",
    "    eval_dataloader = DataLoader(\n",
    "        dataset=dataset[\"test\"].take(100),\n",
    "        batch_size=1,\n",
    "        collate_fn=DataCollator,\n",
    "        num_workers=0)\n",
    "    \n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01, eps=1e-6, betas=(0.9, 0.98))\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.25, total_iters=10000, last_epoch=-1)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        \n",
    "    train_and_evaluate(model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        train_loader=train_dataloader, \n",
    "        eval_loader=eval_dataloader, \n",
    "        optimizer=optimizer, \n",
    "        scheduler=scheduler, \n",
    "        loss_fn=loss_fn, \n",
    "        max_steps=10000,\n",
    "        device='cuda', \n",
    "        accumulation_steps=1, \n",
    "        clear_cache=False, \n",
    "        log_interval=10, \n",
    "        eval_interval=500, \n",
    "        save_interval=10000, \n",
    "        checkpoint_dir=checkpoint_dir, \n",
    "        log_dir=log_dir\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import program\n",
    "log_dir = \"./output/logs\" \n",
    "tb = program.TensorBoard()\n",
    "tb.configure(argv=[None, '--logdir', log_dir])\n",
    "url = tb.launch()\n",
    "print(f\"TensorBoard started at {url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
